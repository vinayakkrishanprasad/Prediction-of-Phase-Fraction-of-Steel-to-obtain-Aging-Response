{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# from sklearn.metrics import confusion_matrix , classification_report\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.random.set_seed(1234)\n",
    "import os\n",
    "import random\n",
    "from keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(42)\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>X11</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "      <th>Unnamed: 12</th>\n",
       "      <th>Unnamed: 13</th>\n",
       "      <th>Unnamed: 14</th>\n",
       "      <th>Unnamed: 15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.54</td>\n",
       "      <td>5.67</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.003</td>\n",
       "      <td>68.98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.993</td>\n",
       "      <td>0.2</td>\n",
       "      <td>566</td>\n",
       "      <td>4.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17.54</td>\n",
       "      <td>5.67</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.003</td>\n",
       "      <td>68.98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.993</td>\n",
       "      <td>0.5</td>\n",
       "      <td>566</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.54</td>\n",
       "      <td>5.67</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.003</td>\n",
       "      <td>68.98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.993</td>\n",
       "      <td>1.0</td>\n",
       "      <td>566</td>\n",
       "      <td>17.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.54</td>\n",
       "      <td>5.67</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.003</td>\n",
       "      <td>68.98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.993</td>\n",
       "      <td>3.0</td>\n",
       "      <td>566</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.54</td>\n",
       "      <td>5.67</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.003</td>\n",
       "      <td>68.98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.993</td>\n",
       "      <td>7.0</td>\n",
       "      <td>566</td>\n",
       "      <td>34.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      X1    X2   X3     X4     X5   X6   X7      X8   X9  X10   X11  \\\n",
       "0  17.54  5.67  7.8  0.003  68.98  0.0  0.0  99.993  0.2  566   4.3   \n",
       "1  17.54  5.67  7.8  0.003  68.98  0.0  0.0  99.993  0.5  566  10.0   \n",
       "2  17.54  5.67  7.8  0.003  68.98  0.0  0.0  99.993  1.0  566  17.5   \n",
       "3  17.54  5.67  7.8  0.003  68.98  0.0  0.0  99.993  3.0  566  31.0   \n",
       "4  17.54  5.67  7.8  0.003  68.98  0.0  0.0  99.993  7.0  566  34.0   \n",
       "\n",
       "   Unnamed: 11  Unnamed: 12  Unnamed: 13  Unnamed: 14  Unnamed: 15  \n",
       "0          NaN          NaN          NaN          NaN          NaN  \n",
       "1          NaN          NaN          NaN          NaN          NaN  \n",
       "2          NaN          NaN          NaN          NaN          NaN  \n",
       "3          NaN          NaN          NaN          NaN          NaN  \n",
       "4          NaN          NaN          NaN          NaN          NaN  "
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_excel(\"data.xlsx\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= df[['X1','X2','X3','X4','X5','X6','X7','X8','X9','X10','X11']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>X11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.54</td>\n",
       "      <td>5.67</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.003</td>\n",
       "      <td>68.98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.993</td>\n",
       "      <td>0.2</td>\n",
       "      <td>566</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17.54</td>\n",
       "      <td>5.67</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.003</td>\n",
       "      <td>68.98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.993</td>\n",
       "      <td>0.5</td>\n",
       "      <td>566</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.54</td>\n",
       "      <td>5.67</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.003</td>\n",
       "      <td>68.98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.993</td>\n",
       "      <td>1.0</td>\n",
       "      <td>566</td>\n",
       "      <td>17.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.54</td>\n",
       "      <td>5.67</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.003</td>\n",
       "      <td>68.98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.993</td>\n",
       "      <td>3.0</td>\n",
       "      <td>566</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.54</td>\n",
       "      <td>5.67</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.003</td>\n",
       "      <td>68.98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.993</td>\n",
       "      <td>7.0</td>\n",
       "      <td>566</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      X1    X2   X3     X4     X5   X6   X7      X8   X9  X10   X11\n",
       "0  17.54  5.67  7.8  0.003  68.98  0.0  0.0  99.993  0.2  566   4.3\n",
       "1  17.54  5.67  7.8  0.003  68.98  0.0  0.0  99.993  0.5  566  10.0\n",
       "2  17.54  5.67  7.8  0.003  68.98  0.0  0.0  99.993  1.0  566  17.5\n",
       "3  17.54  5.67  7.8  0.003  68.98  0.0  0.0  99.993  3.0  566  31.0\n",
       "4  17.54  5.67  7.8  0.003  68.98  0.0  0.0  99.993  7.0  566  34.0"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= data[['X1','X2','X3','X4','X5','X6','X7','X8','X9','X10']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.54</td>\n",
       "      <td>5.67</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.003</td>\n",
       "      <td>68.98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.993</td>\n",
       "      <td>0.2</td>\n",
       "      <td>566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17.54</td>\n",
       "      <td>5.67</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.003</td>\n",
       "      <td>68.98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.993</td>\n",
       "      <td>0.5</td>\n",
       "      <td>566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.54</td>\n",
       "      <td>5.67</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.003</td>\n",
       "      <td>68.98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.993</td>\n",
       "      <td>1.0</td>\n",
       "      <td>566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.54</td>\n",
       "      <td>5.67</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.003</td>\n",
       "      <td>68.98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.993</td>\n",
       "      <td>3.0</td>\n",
       "      <td>566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.54</td>\n",
       "      <td>5.67</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.003</td>\n",
       "      <td>68.98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.993</td>\n",
       "      <td>7.0</td>\n",
       "      <td>566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      X1    X2   X3     X4     X5   X6   X7      X8   X9  X10\n",
       "0  17.54  5.67  7.8  0.003  68.98  0.0  0.0  99.993  0.2  566\n",
       "1  17.54  5.67  7.8  0.003  68.98  0.0  0.0  99.993  0.5  566\n",
       "2  17.54  5.67  7.8  0.003  68.98  0.0  0.0  99.993  1.0  566\n",
       "3  17.54  5.67  7.8  0.003  68.98  0.0  0.0  99.993  3.0  566\n",
       "4  17.54  5.67  7.8  0.003  68.98  0.0  0.0  99.993  7.0  566"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y= data['X11']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     4.3\n",
       "1    10.0\n",
       "2    17.5\n",
       "3    31.0\n",
       "4    34.0\n",
       "Name: X11, dtype: float64"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "17\n",
      "17\n",
      "8\n",
      "18\n",
      "17\n",
      "9\n",
      "14\n",
      "55\n",
      "52\n"
     ]
    }
   ],
   "source": [
    "#We check the number of unique values in each column\n",
    "a=['X1','X2','X3','X4','X5','X6','X7','X8','X9','X10']\n",
    "for i in a:\n",
    "    print(len(X[i].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the boxplot for each variable\n",
    "# # subplots(): plot subplots\n",
    "# # figsize(): set the figure size\n",
    "# fig, ax = plt.subplots(2, 5, figsize=(15, 8))\n",
    "\n",
    "# # plot the boxplot using boxplot() from seaborn\n",
    "# # z: let the variable z define the boxplot\n",
    "# # x: data for which the boxplot is to be plotted\n",
    "# # orient: \"h\" specifies horizontal boxplot (for vertical boxplots use \"v\")\n",
    "# # whis: proportion of the IQR past the low and high quartiles to extend the plot whiskers\n",
    "# # ax: specifies the axes object to draw the plot o\n",
    "# # set_xlabel(): set the x-axis label\n",
    "# # fontsize: sets the font size of the x-axis label\n",
    "# for variable, subplot in zip(X.columns, ax.flatten()):\n",
    "#     z = sns.boxplot(x = X[variable], orient = \"h\",whis=1.5 , ax=subplot) # plot the boxplot\n",
    "#     z.set_xlabel(variable, fontsize = 20)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #this code reduces the  above outliers seen. This is done by chaning the values in the box plot based on inter quantile range \n",
    "# for i in X.columns:\n",
    "#     q1=X[i].quantile(0.25)\n",
    "#     q3=X[i].quantile(0.75)\n",
    "#     iqr=q3-q1\n",
    "#     ub=q3 + 1.5*iqr\n",
    "#     lb=q1 - 1.5*iqr\n",
    "#     uc=X[i].quantile(0.99)\n",
    "#     lc=X[i].quantile(0.01)\n",
    "#     for ind1 in X[i].index:\n",
    "#         if X.loc[ind1, i] >ub:            \n",
    "#             X.loc[ind1, i] =uc\n",
    "#         if X.loc[ind1, i] < lb:\n",
    "#             X.loc[ind1, i] =lc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the boxplot for each variable\n",
    "# # subplots(): plot subplots\n",
    "# # figsize(): set the figure size\n",
    "# fig, ax = plt.subplots(2, 5, figsize=(15, 8))\n",
    "\n",
    "# # plot the boxplot using boxplot() from seaborn\n",
    "# # z: let the variable z define the boxplot\n",
    "# # x: data for which the boxplot is to be plotted\n",
    "# # orient: \"h\" specifies horizontal boxplot (for vertical boxplots use \"v\")\n",
    "# # whis: proportion of the IQR past the low and high quartiles to extend the plot whiskers\n",
    "# # ax: specifies the axes object to draw the plot o\n",
    "# # set_xlabel(): set the x-axis label\n",
    "# # fontsize: sets the font size of the x-axis label\n",
    "# for variable, subplot in zip(X.columns, ax.flatten()):\n",
    "#     z = sns.boxplot(x = X[variable], orient = \"h\",whis=1.5 , ax=subplot) # plot the boxplot\n",
    "#     z.set_xlabel(variable, fontsize = 20)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler(with_std  = True ,with_mean = True, copy = True)\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras import optimizers\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, roc_curve, auc\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf =KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean_RF_Acc :  0.8069808666979871\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X, Y)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "RF_accuracies = cross_val_score(estimator = rf, X = X, y = Y, cv = kf,scoring=\"r2\")\n",
    "print(\"Mean_RF_Acc : \", RF_accuracies.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So i just tried a neural network below, it is similar to the neural network given in the machine learning mastery, not really sure what it means but have to work on that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DO NOT Modify this gives 74% accuracy\n",
    "# reg_model = Sequential()\n",
    "# a=reg_model.add(Dense(8, input_dim=10, activation='relu',kernel_initializer='he_uniform',kernel_regularizer='l2'))\n",
    "# # reg_model.add(Dense(4, activation='relu',kernel_regularizer='l2'))\n",
    "# reg_model.add(Dropout(0.2))\n",
    "# reg_model.add(Dense(1, activation='linear'))\n",
    "# reg_model.compile(loss='mae', \n",
    "#                 optimizer='SGD')\n",
    "\n",
    "\n",
    "# history = reg_model.fit(X_train_std, Y_train, \n",
    "#                             validation_data=(X_test_std, Y_test), \n",
    "#                             epochs=100, verbose=1)\n",
    "\n",
    "# train_mse = reg_model.evaluate(X_train_std, Y_train, verbose=0)\n",
    "# test_mse = reg_model.evaluate(X_test_std, Y_test, verbose=0)\n",
    "# print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))\n",
    "# # plot loss during training\n",
    "# plt.title('Loss / Mean Squared Error')\n",
    "# plt.plot(history.history['loss'], label='train')\n",
    "# plt.plot(history.history['val_loss'], label='test')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DO NOT Modify this gives 74% accuracy\n",
    "# reg_model = Sequential()\n",
    "# a=reg_model.add(Dense(8, input_dim=10, activation='relu',kernel_initializer='he_uniform',kernel_regularizer='l2'))\n",
    "# # reg_model.add(Dense(4, activation='relu',kernel_regularizer='l2'))\n",
    "# reg_model.add(Dropout(0.2))\n",
    "# reg_model.add(Dense(1, activation='linear'))\n",
    "# reg_model.compile(loss='mae', \n",
    "#                 optimizer='SGD')\n",
    "\n",
    "\n",
    "# history = reg_model.fit(X_train_std, Y_train, \n",
    "#                             validation_data=(X_test_std, Y_test), \n",
    "#                             epochs=100, verbose=1)\n",
    "\n",
    "# train_mse = reg_model.evaluate(X_train_std, Y_train, verbose=0)\n",
    "# test_mse = reg_model.evaluate(X_test_std, Y_test, verbose=0)\n",
    "# print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))\n",
    "# # plot loss during training\n",
    "# plt.title('Loss / Mean Squared Error')\n",
    "# plt.plot(history.history['loss'], label='train')\n",
    "# plt.plot(history.history['val_loss'], label='test')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DO NOT Modify this gives 74% accuracy\n",
    "# reg_model = Sequential()\n",
    "# a=reg_model.add(Dense(8, input_dim=10, activation='relu',kernel_initializer='he_uniform',kernel_regularizer='l2'))\n",
    "# # reg_model.add(Dense(4, activation='relu',kernel_regularizer='l2'))\n",
    "# reg_model.add(Dropout(0.2))\n",
    "# reg_model.add(Dense(1, activation='linear'))\n",
    "# reg_model.compile(loss='mae', \n",
    "#                 optimizer='SGD')\n",
    "\n",
    "\n",
    "# history = reg_model.fit(X_train_std, Y_train, \n",
    "#                             validation_data=(X_test_std, Y_test), \n",
    "#                             epochs=100, verbose=1)\n",
    "\n",
    "# train_mse = reg_model.evaluate(X_train_std, Y_train, verbose=0)\n",
    "# test_mse = reg_model.evaluate(X_test_std, Y_test, verbose=0)\n",
    "# print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))\n",
    "# # plot loss during training\n",
    "# plt.title('Loss / Mean Squared Error')\n",
    "# plt.plot(history.history['loss'], label='train')\n",
    "# plt.plot(history.history['val_loss'], label='test')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 10.7496 - val_loss: 13.3279\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 10.4953 - val_loss: 12.9435\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0333 - val_loss: 12.4168\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4366 - val_loss: 11.6309\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6508 - val_loss: 10.4462\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8480 - val_loss: 9.1323\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.6740 - val_loss: 8.6303\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.3679 - val_loss: 8.0387\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6562 - val_loss: 7.0553\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.8257 - val_loss: 6.4971\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4.5037 - val_loss: 6.2739\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4.4928 - val_loss: 6.5996\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.4452 - val_loss: 6.2412\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4.2366 - val_loss: 5.9840\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.2813 - val_loss: 5.9095\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.9874 - val_loss: 5.9199\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.2160 - val_loss: 5.8678\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.0766 - val_loss: 5.9172\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 3.9252 - val_loss: 5.8624\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4.2197 - val_loss: 5.9793\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.2237 - val_loss: 5.7176\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.1916 - val_loss: 5.7563\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.0827 - val_loss: 6.0516\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.0501 - val_loss: 5.4327\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3445 - val_loss: 5.8002\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.0300 - val_loss: 5.5069\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.9649 - val_loss: 5.5464\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.0827 - val_loss: 5.6704\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.0652 - val_loss: 5.6512\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4.0235 - val_loss: 5.4653\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.7511 - val_loss: 5.3821\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.9808 - val_loss: 5.4655\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7110 - val_loss: 5.5006\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8613 - val_loss: 5.4342\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.8562 - val_loss: 5.5681\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.9182 - val_loss: 5.3487\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7613 - val_loss: 5.3597\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5614 - val_loss: 5.4893\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.9049 - val_loss: 5.4001\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.9375 - val_loss: 5.4743\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.9677 - val_loss: 5.2759\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7242 - val_loss: 5.4365\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.8347 - val_loss: 5.4767\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7499 - val_loss: 5.3398\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.0979 - val_loss: 5.2576\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7996 - val_loss: 5.3911\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8541 - val_loss: 5.4666\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.8756 - val_loss: 5.2643\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.9009 - val_loss: 5.4447\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7527 - val_loss: 5.2033\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.0297 - val_loss: 5.1549\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.1839 - val_loss: 5.3518\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.2079 - val_loss: 5.6202\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.0079 - val_loss: 5.7036\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.0712 - val_loss: 5.3765\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.0919 - val_loss: 5.5025\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.9094 - val_loss: 5.4053\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7806 - val_loss: 5.4397\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7719 - val_loss: 5.2869\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8276 - val_loss: 5.4061\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7208 - val_loss: 5.1634\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.9404 - val_loss: 5.3207\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7608 - val_loss: 5.4236\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8404 - val_loss: 5.2981\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4650 - val_loss: 5.3596\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8300 - val_loss: 5.4459\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6534 - val_loss: 5.1579\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7007 - val_loss: 5.2141\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.7659 - val_loss: 5.3245\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 3.8522 - val_loss: 5.2669\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6276 - val_loss: 5.1807\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6444 - val_loss: 5.3547\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5987 - val_loss: 5.1721\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6363 - val_loss: 5.2872\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8030 - val_loss: 5.2453\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8421 - val_loss: 5.1945\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6283 - val_loss: 5.3347\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6238 - val_loss: 5.3336\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8599 - val_loss: 5.1801\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 3.9170 - val_loss: 5.0822\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7979 - val_loss: 5.3937\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5349 - val_loss: 5.0427\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7599 - val_loss: 5.2069\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 6ms/step - loss: 3.7388 - val_loss: 5.1560\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8778 - val_loss: 4.9856\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6923 - val_loss: 5.0772\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7401 - val_loss: 5.0855\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.8286 - val_loss: 5.2180\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5157 - val_loss: 5.1503\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5441 - val_loss: 4.9627\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.5856 - val_loss: 5.1922\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5103 - val_loss: 5.0834\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6240 - val_loss: 4.8194\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 4.147 - 0s 4ms/step - loss: 3.7290 - val_loss: 5.0815\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5596 - val_loss: 5.0276\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4446 - val_loss: 4.9966\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4320 - val_loss: 4.9513\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4969 - val_loss: 5.3264\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6181 - val_loss: 4.8512\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5097 - val_loss: 4.8720\n",
      "Train: 3.441, Test: 4.872\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # DO NOT Modify this gives 74% accuracy- LOL just kidding\n",
    "# reg_model = Sequential()\n",
    "# a=reg_model.add(Dense(8, input_dim=10, activation='relu',kernel_initializer='he_uniform',kernel_regularizer='l2'))\n",
    "# # reg_model.add(Dense(4, activation='relu',kernel_regularizer='l2'))\n",
    "# reg_model.add(Dropout(0.2))\n",
    "# reg_model.add(Dense(1, activation='linear'))\n",
    "# reg_model.compile(loss='mae', \n",
    "#                 optimizer='SGD')\n",
    "\n",
    "\n",
    "# history = reg_model.fit(X_train_std, Y_train, \n",
    "#                             validation_data=(X_test_std, Y_test), \n",
    "#                             epochs=100, verbose=1)\n",
    "\n",
    "# train_mse = reg_model.evaluate(X_train_std, Y_train, verbose=0)\n",
    "# test_mse = reg_model.evaluate(X_test_std, Y_test, verbose=0)\n",
    "# print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))\n",
    "# # plot loss during training\n",
    "# plt.title('Loss / Mean Squared Error')\n",
    "# plt.plot(history.history['loss'], label='train')\n",
    "# plt.plot(history.history['val_loss'], label='test')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "# # DO NOT Modify this gives 70%- actually\n",
    "reg_model = Sequential()\n",
    "a=reg_model.add(Dense(8, input_dim=10, activation=LeakyReLU(),kernel_initializer='normal'))\n",
    "reg_model.add(Dense(4, activation=LeakyReLU()))\n",
    "reg_model.add(Dropout(0.05))\n",
    "reg_model.add(Dense(1, activation=LeakyReLU()))\n",
    "reg_model.compile(loss='mae', \n",
    "                optimizer=opt)\n",
    "\n",
    "\n",
    "history = reg_model.fit(X_train_std, Y_train, \n",
    "                            validation_data=(X_test_std, Y_test), \n",
    "                            epochs=100, verbose=1)\n",
    "y_pred=reg_model.predict(X_test_std)\n",
    "train_mse = reg_model.evaluate(X_train_std, Y_train, verbose=0)\n",
    "test_mse = reg_model.evaluate(X_test_std, Y_test, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))\n",
    "# plot loss during training\n",
    "plt.title('Loss / Mean Squared Error')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7110123817460635"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(Y_test,y_pred)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6894842435160897"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n= len(X_train_std)\n",
    "p = len(X.columns)\n",
    "adj_R2 = 1- ((1-R2) * (n-1)/(n-p-1)) #Adj R2 = 1-(1-R2)*(n-1)/(n-p-1)\n",
    "adj_R2**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X= scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#principal component analysis\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.99)\n",
    "X_pca = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(219, 8)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.644530</td>\n",
       "      <td>0.561085</td>\n",
       "      <td>-0.056426</td>\n",
       "      <td>-0.053508</td>\n",
       "      <td>0.150036</td>\n",
       "      <td>-0.029116</td>\n",
       "      <td>-0.121246</td>\n",
       "      <td>-0.029008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.644541</td>\n",
       "      <td>0.561058</td>\n",
       "      <td>-0.056457</td>\n",
       "      <td>-0.053537</td>\n",
       "      <td>0.150010</td>\n",
       "      <td>-0.029114</td>\n",
       "      <td>-0.121294</td>\n",
       "      <td>-0.028705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.644560</td>\n",
       "      <td>0.561013</td>\n",
       "      <td>-0.056509</td>\n",
       "      <td>-0.053584</td>\n",
       "      <td>0.149968</td>\n",
       "      <td>-0.029109</td>\n",
       "      <td>-0.121374</td>\n",
       "      <td>-0.028199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.644633</td>\n",
       "      <td>0.560831</td>\n",
       "      <td>-0.056718</td>\n",
       "      <td>-0.053774</td>\n",
       "      <td>0.149797</td>\n",
       "      <td>-0.029090</td>\n",
       "      <td>-0.121694</td>\n",
       "      <td>-0.026176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.644781</td>\n",
       "      <td>0.560468</td>\n",
       "      <td>-0.057136</td>\n",
       "      <td>-0.054154</td>\n",
       "      <td>0.149455</td>\n",
       "      <td>-0.029052</td>\n",
       "      <td>-0.122335</td>\n",
       "      <td>-0.022130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -0.644530  0.561085 -0.056426 -0.053508  0.150036 -0.029116 -0.121246   \n",
       "1 -0.644541  0.561058 -0.056457 -0.053537  0.150010 -0.029114 -0.121294   \n",
       "2 -0.644560  0.561013 -0.056509 -0.053584  0.149968 -0.029109 -0.121374   \n",
       "3 -0.644633  0.560831 -0.056718 -0.053774  0.149797 -0.029090 -0.121694   \n",
       "4 -0.644781  0.560468 -0.057136 -0.054154  0.149455 -0.029052 -0.122335   \n",
       "\n",
       "          7  \n",
       "0 -0.029008  \n",
       "1 -0.028705  \n",
       "2 -0.028199  \n",
       "3 -0.026176  \n",
       "4 -0.022130  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PCA_df = pd.DataFrame(data = X_pca)\n",
    "PCA_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(PCA_df, Y, test_size=0.20, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
    "from sklearn import linear_model, tree, ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step - loss: 8.6978 - mae: 8.6978\n",
      "[8.697833061218262, 8.697833061218262]\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000021B46B58DC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "# DO NOT Modify\n",
    "reg_model = Sequential()\n",
    "reg_model.add(Dense(8, input_dim=8, activation='relu',kernel_initializer='he_uniform'))\n",
    "reg_model.add(Dense(4, activation='relu'))\n",
    "reg_model.add(Dropout(0.2))\n",
    "\n",
    "reg_model.add(Dense(1, activation='linear'))\n",
    "reg_model.compile(loss='mae', \n",
    "                optimizer='SGD', \n",
    "                metrics=['mae'])\n",
    "\n",
    "\n",
    "his = reg_model.fit(X_train, Y_train, \n",
    "                            validation_data=(X_test, Y_test), \n",
    "                            epochs=100, verbose=0)\n",
    "\n",
    "print(reg_model.evaluate(X_test, Y_test))\n",
    "y_preds = reg_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2=r2_score(Y_test,y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.11505813192651204"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n= len(X_train)\n",
    "p = len(X[1])\n",
    "adj_R2 = 1- ((1-R2) * (n-1)/(n-p-1)) #Adj R2 = 1-(1-R2)*(n-1)/(n-p-1)\n",
    "adj_R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoEncoder - standard scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler(with_std  = True ,with_mean = True, copy = True)\n",
    "X = sc.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Epoch 2/100\n",
      "Epoch 3/100\n",
      "Epoch 4/100\n",
      "Epoch 5/100\n",
      "Epoch 6/100\n",
      "Epoch 7/100\n",
      "Epoch 8/100\n",
      "Epoch 9/100\n",
      "Epoch 10/100\n",
      "Epoch 11/100\n",
      "Epoch 12/100\n",
      "Epoch 13/100\n",
      "Epoch 14/100\n",
      "Epoch 15/100\n",
      "Epoch 16/100\n",
      "Epoch 17/100\n",
      "Epoch 18/100\n",
      "Epoch 19/100\n",
      "Epoch 20/100\n",
      "Epoch 21/100\n",
      "Epoch 22/100\n",
      "Epoch 23/100\n",
      "Epoch 24/100\n",
      "Epoch 25/100\n",
      "Epoch 26/100\n",
      "Epoch 27/100\n",
      "Epoch 28/100\n",
      "Epoch 29/100\n",
      "Epoch 30/100\n",
      "Epoch 31/100\n",
      "Epoch 32/100\n",
      "Epoch 33/100\n",
      "Epoch 34/100\n",
      "Epoch 35/100\n",
      "Epoch 36/100\n",
      "Epoch 37/100\n",
      "Epoch 38/100\n",
      "Epoch 39/100\n",
      "Epoch 40/100\n",
      "Epoch 41/100\n",
      "Epoch 42/100\n",
      "Epoch 43/100\n",
      "Epoch 44/100\n",
      "Epoch 45/100\n",
      "Epoch 46/100\n",
      "Epoch 47/100\n",
      "Epoch 48/100\n",
      "Epoch 49/100\n",
      "Epoch 50/100\n",
      "Epoch 51/100\n",
      "Epoch 52/100\n",
      "Epoch 53/100\n",
      "Epoch 54/100\n",
      "Epoch 55/100\n",
      "Epoch 56/100\n",
      "Epoch 57/100\n",
      "Epoch 58/100\n",
      "Epoch 59/100\n",
      "Epoch 60/100\n",
      "Epoch 61/100\n",
      "Epoch 62/100\n",
      "Epoch 63/100\n",
      "Epoch 64/100\n",
      "Epoch 65/100\n",
      "Epoch 66/100\n",
      "Epoch 67/100\n",
      "Epoch 68/100\n",
      "Epoch 69/100\n",
      "Epoch 70/100\n",
      "Epoch 71/100\n",
      "Epoch 72/100\n",
      "Epoch 73/100\n",
      "Epoch 74/100\n",
      "Epoch 75/100\n",
      "Epoch 76/100\n",
      "Epoch 77/100\n",
      "Epoch 78/100\n",
      "Epoch 79/100\n",
      "Epoch 80/100\n",
      "Epoch 81/100\n",
      "Epoch 82/100\n",
      "Epoch 83/100\n",
      "Epoch 84/100\n",
      "Epoch 85/100\n",
      "Epoch 86/100\n",
      "Epoch 87/100\n",
      "Epoch 88/100\n",
      "Epoch 89/100\n",
      "Epoch 90/100\n",
      "Epoch 91/100\n",
      "Epoch 92/100\n",
      "Epoch 93/100\n",
      "Epoch 94/100\n",
      "Epoch 95/100\n",
      "Epoch 96/100\n",
      "Epoch 97/100\n",
      "Epoch 98/100\n",
      "Epoch 99/100\n",
      "Epoch 100/100\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "input_layer = Input(shape=(X.shape[1],))\n",
    "encoded = Dense(10, activation='relu')(input_layer)\n",
    "decoded = Dense(X.shape[1], activation='softmax')(encoded)\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "X1, X2, Y1, Y2 = train_test_split(X, X, test_size=0.3, random_state=101)\n",
    "\n",
    "autoencoder.fit(X1, Y1,\n",
    "                epochs=100,\n",
    "                shuffle=True,\n",
    "                verbose = 30,\n",
    "                validation_data=(X2, Y2))\n",
    "\n",
    "encoder = Model(input_layer, encoded)\n",
    "X_ae = encoder.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5345026 , 0.6821753 , 1.0793736 , ..., 0.        , 3.9087536 ,\n",
       "        1.5296675 ],\n",
       "       [0.533669  , 0.68316996, 1.0797305 , ..., 0.        , 3.9093466 ,\n",
       "        1.5279753 ],\n",
       "       [0.53227955, 0.68482786, 1.0803254 , ..., 0.        , 3.9103346 ,\n",
       "        1.5251552 ],\n",
       "       ...,\n",
       "       [0.        , 2.286841  , 0.        , ..., 2.6665328 , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 2.300561  , 0.        , ..., 2.6602683 , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 2.3280005 , 0.        , ..., 2.6477394 , 0.        ,\n",
       "        0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_ae, Y, test_size=0.20, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 11.2535 - val_loss: 13.7608\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.0335 - val_loss: 13.6155\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.9029 - val_loss: 13.5103\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8037 - val_loss: 13.4372\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.7337 - val_loss: 13.3790\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6746 - val_loss: 13.3299\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 10.6252 - val_loss: 13.2890\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 10.5818 - val_loss: 13.2520\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.5407 - val_loss: 13.2171\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.5018 - val_loss: 13.1898\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.4677 - val_loss: 13.1626\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 10.4330 - val_loss: 13.1353\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.3981 - val_loss: 13.1075\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.3635 - val_loss: 13.0804\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.3295 - val_loss: 13.0532\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.2962 - val_loss: 13.0257\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.2639 - val_loss: 12.9999\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 10.2330 - val_loss: 12.9730\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.2022 - val_loss: 12.9483\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.1744 - val_loss: 12.9232\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.1465 - val_loss: 12.9007\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.1221 - val_loss: 12.8765\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.0965 - val_loss: 12.8523\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.0717 - val_loss: 12.8285\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.0472 - val_loss: 12.8049\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 10.0233 - val_loss: 12.7809\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.9996 - val_loss: 12.7574\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.9770 - val_loss: 12.7348\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.9553 - val_loss: 12.7127\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9351 - val_loss: 12.6895\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9142 - val_loss: 12.6703\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8954 - val_loss: 12.6496\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.8753 - val_loss: 12.6308\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 9.8555 - val_loss: 12.6104\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8350 - val_loss: 12.5905\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 9.8151 - val_loss: 12.5709\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7953 - val_loss: 12.5505\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.7751 - val_loss: 12.5310\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.7561 - val_loss: 12.5091\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.7343 - val_loss: 12.4894\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.7153 - val_loss: 12.4687\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6964 - val_loss: 12.4496\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.6795 - val_loss: 12.4307\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6625 - val_loss: 12.4103\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.6455 - val_loss: 12.3874\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6275 - val_loss: 12.3692\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6127 - val_loss: 12.3490\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5979 - val_loss: 12.3296\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5814 - val_loss: 12.3062\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5648 - val_loss: 12.2850\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.5478 - val_loss: 12.2602\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.5306 - val_loss: 12.2364\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.5124 - val_loss: 12.2129\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4960 - val_loss: 12.1844\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4771 - val_loss: 12.1572\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4593 - val_loss: 12.1307\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4433 - val_loss: 12.1085\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 9.4285 - val_loss: 12.0857\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4146 - val_loss: 12.0629\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4012 - val_loss: 12.0352\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3866 - val_loss: 12.0094\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3719 - val_loss: 11.9923\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3604 - val_loss: 11.9662\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3458 - val_loss: 11.9406\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.3313 - val_loss: 11.9096\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 9.3154 - val_loss: 11.8782\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.2975 - val_loss: 11.8418\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.2797 - val_loss: 11.8177\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.2654 - val_loss: 11.7778\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2449 - val_loss: 11.7497\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.2326 - val_loss: 11.7275\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2158 - val_loss: 11.6897\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.1969 - val_loss: 11.6490\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1777 - val_loss: 11.6072\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.1576 - val_loss: 11.5634\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1370 - val_loss: 11.5254\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.1178 - val_loss: 11.4783\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.0939 - val_loss: 11.4350\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0714 - val_loss: 11.3798\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.0406 - val_loss: 11.3304\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0100 - val_loss: 11.2726\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.9788 - val_loss: 11.2029\n",
      "Epoch 83/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 8.9416 - val_loss: 11.1466\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.9069 - val_loss: 11.0855\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8702 - val_loss: 11.0321\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8421 - val_loss: 10.9846\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8121 - val_loss: 10.9232\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7749 - val_loss: 10.8526\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7419 - val_loss: 10.7851\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7075 - val_loss: 10.7076\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.6653 - val_loss: 10.6304\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6272 - val_loss: 10.5652\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.5978 - val_loss: 10.4995\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5538 - val_loss: 10.4360\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5106 - val_loss: 10.3579\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4699 - val_loss: 10.2875\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4273 - val_loss: 10.1987\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 8.3877 - val_loss: 10.1454\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3481 - val_loss: 10.1214\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3221 - val_loss: 10.0694\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2818 - val_loss: 10.0166\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2571 - val_loss: 9.9524\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2192 - val_loss: 9.9091\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.1799 - val_loss: 9.8658\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.1386 - val_loss: 9.8073\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.0975 - val_loss: 9.7838\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.0554 - val_loss: 9.7173\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.0162 - val_loss: 9.6674\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9673 - val_loss: 9.6597\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9104 - val_loss: 9.6177\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8554 - val_loss: 9.5967\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7993 - val_loss: 9.5697\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7562 - val_loss: 9.5220\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6869 - val_loss: 9.4943\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6477 - val_loss: 9.4817\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.5903 - val_loss: 9.3960\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.5653 - val_loss: 9.3398\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.5274 - val_loss: 9.2854\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.4923 - val_loss: 9.2919\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.4756 - val_loss: 9.2551\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.4474 - val_loss: 9.2163\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.4180 - val_loss: 9.1996\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.3760 - val_loss: 9.1181\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.3521 - val_loss: 9.1013\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.3057 - val_loss: 9.0087\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.2698 - val_loss: 8.9993\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.2549 - val_loss: 8.9158\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.2116 - val_loss: 8.9011\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 7.1688 - val_loss: 8.8668\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.1398 - val_loss: 8.8374\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.1047 - val_loss: 8.7814\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.0754 - val_loss: 8.7298\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 7.0155 - val_loss: 8.7029\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9940 - val_loss: 8.6187\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9294 - val_loss: 8.6677\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 6.8785 - val_loss: 8.5487\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.8043 - val_loss: 8.5059\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7580 - val_loss: 8.4148\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7198 - val_loss: 8.3294\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6876 - val_loss: 8.2458\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6271 - val_loss: 8.2423\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5752 - val_loss: 8.1265\n",
      "Epoch 143/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.4888 - val_loss: 8.0566\n",
      "Epoch 144/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4444 - val_loss: 8.0339\n",
      "Epoch 145/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.3902 - val_loss: 7.9778\n",
      "Epoch 146/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.3051 - val_loss: 7.9716\n",
      "Epoch 147/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.2720 - val_loss: 7.8725\n",
      "Epoch 148/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.2038 - val_loss: 7.9576\n",
      "Epoch 149/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.1955 - val_loss: 7.8694\n",
      "Epoch 150/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.1252 - val_loss: 7.7909\n",
      "Epoch 151/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.0510 - val_loss: 7.6856\n",
      "Epoch 152/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.0418 - val_loss: 7.8299\n",
      "Epoch 153/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.0018 - val_loss: 7.7962\n",
      "Epoch 154/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.0443 - val_loss: 7.6099\n",
      "Epoch 155/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.9200 - val_loss: 7.5666\n",
      "Epoch 156/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.8808 - val_loss: 7.6743\n",
      "Epoch 157/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.8771 - val_loss: 7.6565\n",
      "Epoch 158/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.8194 - val_loss: 7.5564\n",
      "Epoch 159/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7009 - val_loss: 7.6272\n",
      "Epoch 160/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.8293 - val_loss: 7.4563\n",
      "Epoch 161/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.8428 - val_loss: 7.5041\n",
      "Epoch 162/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7568 - val_loss: 7.2998\n",
      "Epoch 163/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 5.5571 - val_loss: 7.2114\n",
      "Epoch 164/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6275 - val_loss: 7.2272\n",
      "Epoch 165/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.4939 - val_loss: 7.4505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.6482 - val_loss: 7.1199\n",
      "Epoch 167/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.5053 - val_loss: 7.1389\n",
      "Epoch 168/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.4692 - val_loss: 7.2414\n",
      "Epoch 169/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5156 - val_loss: 7.1063\n",
      "Epoch 170/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5041 - val_loss: 7.2479\n",
      "Epoch 171/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5628 - val_loss: 7.3848\n",
      "Epoch 172/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5648 - val_loss: 7.1193\n",
      "Epoch 173/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.3936 - val_loss: 7.1705\n",
      "Epoch 174/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 5.4075 - val_loss: 6.9576\n",
      "Epoch 175/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 5.3236 - val_loss: 7.0424\n",
      "Epoch 176/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.4672 - val_loss: 6.9512\n",
      "Epoch 177/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.2354 - val_loss: 6.8804\n",
      "Epoch 178/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2361 - val_loss: 6.8195\n",
      "Epoch 179/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2008 - val_loss: 7.1475\n",
      "Epoch 180/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.2518 - val_loss: 6.8911\n",
      "Epoch 181/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.1864 - val_loss: 6.8250\n",
      "Epoch 182/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.1308 - val_loss: 6.7515\n",
      "Epoch 183/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0624 - val_loss: 7.1823\n",
      "Epoch 184/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 5.2901 - val_loss: 6.9026\n",
      "Epoch 185/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.2808 - val_loss: 6.7128\n",
      "Epoch 186/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.2076 - val_loss: 6.7058\n",
      "Epoch 187/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.0708 - val_loss: 6.7586\n",
      "Epoch 188/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.0402 - val_loss: 6.7448\n",
      "Epoch 189/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0670 - val_loss: 6.5275\n",
      "Epoch 190/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9491 - val_loss: 6.8886\n",
      "Epoch 191/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0176 - val_loss: 6.9283\n",
      "Epoch 192/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9850 - val_loss: 6.6237\n",
      "Epoch 193/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9657 - val_loss: 6.5348\n",
      "Epoch 194/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.8671 - val_loss: 6.3988\n",
      "Epoch 195/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9713 - val_loss: 6.8154\n",
      "Epoch 196/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.2076 - val_loss: 6.6117\n",
      "Epoch 197/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8056 - val_loss: 6.4066\n",
      "Epoch 198/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8783 - val_loss: 6.3532\n",
      "Epoch 199/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8869 - val_loss: 6.2944\n",
      "Epoch 200/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9621 - val_loss: 6.8096\n",
      "Train: 5.421, Test: 6.810\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DO NOT Modify\n",
    "reg_model = Sequential()\n",
    "reg_model.add(Dense(8, input_dim=10, activation='relu',kernel_regularizer='l2'))\n",
    "reg_model.add(Dense(4, activation='relu',kernel_regularizer='l2'))\n",
    "reg_model.add(Dense(1, activation='linear'))\n",
    "reg_model.compile(loss='mae', \n",
    "                optimizer='SGD')\n",
    "\n",
    "\n",
    "history = reg_model.fit(X_train, Y_train, \n",
    "                            validation_data=(X_test, Y_test), \n",
    "                            epochs=200, verbose=1)\n",
    "\n",
    "train_mse = reg_model.evaluate(X_train, Y_train, verbose=0)\n",
    "test_mse = reg_model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))\n",
    "# plot loss during training\n",
    "y_preds=reg_model.predict(X_test)\n",
    "plt.title('Loss / Mean Squared Error')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5643651644183402"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(Y_test,y_preds)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5262638605599254"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n= len(X_train)\n",
    "p = len(X[1])\n",
    "adj_R2 = 1- ((1-R2) * (n-1)/(n-p-1)) #Adj R2 = 1-(1-R2)*(n-1)/(n-p-1)\n",
    "adj_R2**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "batch_size = 32\n",
    "input_dim = X_train[0].shape[0] #num of predictor variables \n",
    "learning_rate = 1e-4\n",
    "input_layer = Input(shape=(input_dim, ), name=\"input\")\n",
    "#Input Layer\n",
    "encoder = Dense (100, activation=\"relu\", activity_regularizer=regularizers.l1(learning_rate))(input_layer)\n",
    "#Encoders first dense layer\n",
    "encoder = Dense (50, activation=\"relu\",\n",
    "activity_regularizer=regularizers.l1(learning_rate))(encoder)\n",
    "#Encoders second dense layer\n",
    "encoder = Dense (25, activation=\"relu\", activity_regularizer=regularizers.l1(learning_rate))(encoder)\n",
    "# Code layer\n",
    "encoder = Dense (8, activation=\"relu\", activity_regularizer=regularizers.l1(learning_rate))(encoder)\n",
    "# Decoders first dense layer\n",
    "decoder = Dense(25, activation=\"relu\", activity_regularizer=regularizers.l1(learning_rate))(encoder)\n",
    "# Decoders second dense layer\n",
    "decoder = Dense(50, activation=\"relu\", activity_regularizer=regularizers.l1(learning_rate))(decoder)\n",
    "# Decoders Third dense layer\n",
    "decoder = Dense(100, activation=\"relu\", activity_regularizer=regularizers.l1(learning_rate))(decoder)\n",
    "# Output Layer\n",
    "decoder = Dense(input_dim, activation=\"sigmoid\", activity_regularizer=regularizers.l1(learning_rate))(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 2ms/step - loss: 1.2595 - accuracy: 0.1257\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 1.2302 - accuracy: 0.2286\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 1.1804 - accuracy: 0.2286\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.1124 - accuracy: 0.2286\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.0687 - accuracy: 0.2286\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.0378 - accuracy: 0.2286\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.9987 - accuracy: 0.2457\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9568 - accuracy: 0.2686\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9226 - accuracy: 0.2971\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8977 - accuracy: 0.3657\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8877 - accuracy: 0.3829\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8766 - accuracy: 0.3943\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.8680 - accuracy: 0.3314\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8596 - accuracy: 0.3029\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8504 - accuracy: 0.3029\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8409 - accuracy: 0.3029\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8304 - accuracy: 0.3200\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8185 - accuracy: 0.3314\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8092 - accuracy: 0.3714\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7995 - accuracy: 0.4514\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7923 - accuracy: 0.4857\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7862 - accuracy: 0.4857\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7815 - accuracy: 0.4914\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7781 - accuracy: 0.4914\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7754 - accuracy: 0.4971\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7732 - accuracy: 0.5029\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7714 - accuracy: 0.4971\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7699 - accuracy: 0.4971\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7686 - accuracy: 0.4971\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7675 - accuracy: 0.4914\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7664 - accuracy: 0.4914\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7653 - accuracy: 0.4914\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7640 - accuracy: 0.4857\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7629 - accuracy: 0.4800\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7610 - accuracy: 0.4686\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7590 - accuracy: 0.4571\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7572 - accuracy: 0.4457\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7562 - accuracy: 0.4571\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7545 - accuracy: 0.4457\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7533 - accuracy: 0.4571\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7519 - accuracy: 0.4743\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7502 - accuracy: 0.4571\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7479 - accuracy: 0.4686\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7458 - accuracy: 0.4514\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7439 - accuracy: 0.4171\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7421 - accuracy: 0.4000\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7409 - accuracy: 0.4057\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7398 - accuracy: 0.4114\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7389 - accuracy: 0.4057\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7382 - accuracy: 0.3943\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7374 - accuracy: 0.3943\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7368 - accuracy: 0.3943\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7361 - accuracy: 0.3657\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7356 - accuracy: 0.4457\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7350 - accuracy: 0.4400\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7345 - accuracy: 0.4571\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7340 - accuracy: 0.4343\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7334 - accuracy: 0.4400\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7329 - accuracy: 0.4400\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7323 - accuracy: 0.4400\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7318 - accuracy: 0.4400\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7312 - accuracy: 0.4629\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7306 - accuracy: 0.5314\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7300 - accuracy: 0.5314\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7294 - accuracy: 0.5371\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7289 - accuracy: 0.5486\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7284 - accuracy: 0.5600\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7280 - accuracy: 0.5771\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7275 - accuracy: 0.5486\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7271 - accuracy: 0.5486\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7268 - accuracy: 0.5257\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7264 - accuracy: 0.5371\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7260 - accuracy: 0.5429\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7257 - accuracy: 0.5371\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.7253 - accuracy: 0.5429\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7250 - accuracy: 0.5486\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7246 - accuracy: 0.5543\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7241 - accuracy: 0.5771\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7236 - accuracy: 0.5829\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7229 - accuracy: 0.5886\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7210 - accuracy: 0.6000\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.7199 - accuracy: 0.5257\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7182 - accuracy: 0.5600\n",
      "Epoch 84/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7169 - accuracy: 0.5314\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7158 - accuracy: 0.5429\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7152 - accuracy: 0.5314\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7145 - accuracy: 0.5371\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7141 - accuracy: 0.5143\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7136 - accuracy: 0.5257\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7132 - accuracy: 0.5257\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7129 - accuracy: 0.5143\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7126 - accuracy: 0.4971\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7124 - accuracy: 0.4971\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7121 - accuracy: 0.4971\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7119 - accuracy: 0.4971\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7117 - accuracy: 0.4971\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7115 - accuracy: 0.4971\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7113 - accuracy: 0.4914\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7112 - accuracy: 0.5086\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7110 - accuracy: 0.4971\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7109 - accuracy: 0.4971\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7107 - accuracy: 0.4971\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7105 - accuracy: 0.4971\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7103 - accuracy: 0.4800\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7102 - accuracy: 0.4743\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7100 - accuracy: 0.4743\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7099 - accuracy: 0.4743\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7097 - accuracy: 0.4743\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7096 - accuracy: 0.4914\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7094 - accuracy: 0.4743\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7093 - accuracy: 0.4743\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7091 - accuracy: 0.4914\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7090 - accuracy: 0.4114\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7089 - accuracy: 0.4000\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7087 - accuracy: 0.4057\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7086 - accuracy: 0.4229\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7084 - accuracy: 0.4514\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7083 - accuracy: 0.4514\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7081 - accuracy: 0.4857\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7080 - accuracy: 0.4629\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7079 - accuracy: 0.4971\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7078 - accuracy: 0.5143\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7076 - accuracy: 0.4971\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7075 - accuracy: 0.5314\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7074 - accuracy: 0.5200\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7072 - accuracy: 0.5257\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7071 - accuracy: 0.5143\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7070 - accuracy: 0.5314\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7069 - accuracy: 0.5257\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7068 - accuracy: 0.5257\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7066 - accuracy: 0.5371\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7065 - accuracy: 0.5371\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7064 - accuracy: 0.5314\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7063 - accuracy: 0.5429\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7062 - accuracy: 0.5314\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7061 - accuracy: 0.5257\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7061 - accuracy: 0.5486\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7060 - accuracy: 0.5543\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7064 - accuracy: 0.5600\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7060 - accuracy: 0.5257\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7059 - accuracy: 0.5429\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7057 - accuracy: 0.5486\n",
      "Epoch 143/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7057 - accuracy: 0.5600\n",
      "Epoch 144/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7056 - accuracy: 0.5543\n",
      "Epoch 145/200\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.7055 - accuracy: 0.5429\n",
      "Epoch 146/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7053 - accuracy: 0.5600\n",
      "Epoch 147/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7052 - accuracy: 0.5543\n",
      "Epoch 148/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7052 - accuracy: 0.5371\n",
      "Epoch 149/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7053 - accuracy: 0.5429\n",
      "Epoch 150/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7051 - accuracy: 0.5486\n",
      "Epoch 151/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7050 - accuracy: 0.5371\n",
      "Epoch 152/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7049 - accuracy: 0.5371\n",
      "Epoch 153/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7048 - accuracy: 0.5429\n",
      "Epoch 154/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7047 - accuracy: 0.5429\n",
      "Epoch 155/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7046 - accuracy: 0.5371\n",
      "Epoch 156/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7045 - accuracy: 0.5429\n",
      "Epoch 157/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7044 - accuracy: 0.5429\n",
      "Epoch 158/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7044 - accuracy: 0.5371\n",
      "Epoch 159/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7043 - accuracy: 0.5371\n",
      "Epoch 160/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7043 - accuracy: 0.5429\n",
      "Epoch 161/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7042 - accuracy: 0.5429\n",
      "Epoch 162/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7042 - accuracy: 0.5371\n",
      "Epoch 163/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7041 - accuracy: 0.5371\n",
      "Epoch 164/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7040 - accuracy: 0.5429\n",
      "Epoch 165/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7040 - accuracy: 0.5543\n",
      "Epoch 166/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7040 - accuracy: 0.5429\n",
      "Epoch 167/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7039 - accuracy: 0.5429\n",
      "Epoch 168/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7040 - accuracy: 0.5600\n",
      "Epoch 169/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7038 - accuracy: 0.5486\n",
      "Epoch 170/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7038 - accuracy: 0.5429\n",
      "Epoch 171/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7037 - accuracy: 0.5657\n",
      "Epoch 172/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7037 - accuracy: 0.5886\n",
      "Epoch 173/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7036 - accuracy: 0.5486\n",
      "Epoch 174/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7036 - accuracy: 0.5543\n",
      "Epoch 175/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7037 - accuracy: 0.5429\n",
      "Epoch 176/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7035 - accuracy: 0.5543\n",
      "Epoch 177/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7035 - accuracy: 0.5714\n",
      "Epoch 178/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7035 - accuracy: 0.5886\n",
      "Epoch 179/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7036 - accuracy: 0.5486\n",
      "Epoch 180/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7034 - accuracy: 0.5829\n",
      "Epoch 181/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7036 - accuracy: 0.5886\n",
      "Epoch 182/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7033 - accuracy: 0.5600\n",
      "Epoch 183/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7034 - accuracy: 0.5714\n",
      "Epoch 184/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7032 - accuracy: 0.5771\n",
      "Epoch 185/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7031 - accuracy: 0.5829\n",
      "Epoch 186/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7031 - accuracy: 0.5543\n",
      "Epoch 187/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7031 - accuracy: 0.6000\n",
      "Epoch 188/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7030 - accuracy: 0.5714\n",
      "Epoch 189/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7030 - accuracy: 0.5829\n",
      "Epoch 190/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7032 - accuracy: 0.6000\n",
      "Epoch 191/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7031 - accuracy: 0.5829\n",
      "Epoch 192/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7030 - accuracy: 0.6114\n",
      "Epoch 193/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7030 - accuracy: 0.5429\n",
      "Epoch 194/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7028 - accuracy: 0.6229\n",
      "Epoch 195/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7027 - accuracy: 0.6000\n",
      "Epoch 196/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7026 - accuracy: 0.6229\n",
      "Epoch 197/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7025 - accuracy: 0.6000\n",
      "Epoch 198/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7025 - accuracy: 0.6000\n",
      "Epoch 199/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7024 - accuracy: 0.6286\n",
      "Epoch 200/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7024 - accuracy: 0.6229\n"
     ]
    }
   ],
   "source": [
    "autoencoder_1 = Model(inputs=input_layer, outputs=decoder)\n",
    "autoencoder_1.compile(metrics=['accuracy'],loss='mean_squared_error',optimizer='adam')\n",
    "satck_1 = autoencoder_1.fit(X_train, X_train,epochs=200,batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_2_input = autoencoder_1.predict(X_train)\n",
    "autoencoder_2_input = np.concatenate((autoencoder_2_input , X_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 1s 2ms/step - loss: 0.3602 - accuracy: 0.6371\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3563 - accuracy: 0.7514\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3554 - accuracy: 0.6686\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3549 - accuracy: 0.7000\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3545 - accuracy: 0.7200\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3542 - accuracy: 0.7314\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3541 - accuracy: 0.7057\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3539 - accuracy: 0.7429\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3538 - accuracy: 0.7486\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3537 - accuracy: 0.7371\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3536 - accuracy: 0.7543\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3535 - accuracy: 0.7600\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3534 - accuracy: 0.7629\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3534 - accuracy: 0.7857\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3533 - accuracy: 0.7800\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3533 - accuracy: 0.7857\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3532 - accuracy: 0.7800\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3531 - accuracy: 0.7714\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3531 - accuracy: 0.7686\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3531 - accuracy: 0.7800\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3530 - accuracy: 0.7914\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3530 - accuracy: 0.7771\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3529 - accuracy: 0.7857\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3529 - accuracy: 0.7657\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3528 - accuracy: 0.7714\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3528 - accuracy: 0.7743\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3527 - accuracy: 0.7686\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3527 - accuracy: 0.7771\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3526 - accuracy: 0.7771\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3526 - accuracy: 0.7686\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3525 - accuracy: 0.7800\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3525 - accuracy: 0.7743\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3525 - accuracy: 0.7714\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3525 - accuracy: 0.7657\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3524 - accuracy: 0.7714\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3523 - accuracy: 0.7771\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3524 - accuracy: 0.7800\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3523 - accuracy: 0.7743\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3522 - accuracy: 0.7771\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3522 - accuracy: 0.7743\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3521 - accuracy: 0.7771\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3521 - accuracy: 0.7771\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3521 - accuracy: 0.7771\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3521 - accuracy: 0.7714\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3520 - accuracy: 0.7686\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3520 - accuracy: 0.7714\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3520 - accuracy: 0.7714\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3519 - accuracy: 0.7771\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3519 - accuracy: 0.7714\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3521 - accuracy: 0.7629\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3522 - accuracy: 0.7714\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3521 - accuracy: 0.7571\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3519 - accuracy: 0.7657\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3519 - accuracy: 0.7686\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3519 - accuracy: 0.7571\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3518 - accuracy: 0.7686\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3517 - accuracy: 0.7800\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3517 - accuracy: 0.7743\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3517 - accuracy: 0.7686\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3516 - accuracy: 0.7657\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3516 - accuracy: 0.7686\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3515 - accuracy: 0.7657\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3515 - accuracy: 0.7657\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3515 - accuracy: 0.7629\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3514 - accuracy: 0.7743\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3514 - accuracy: 0.7714\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3514 - accuracy: 0.7629\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3514 - accuracy: 0.7714\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3514 - accuracy: 0.7657\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3513 - accuracy: 0.7629\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3513 - accuracy: 0.7657\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3513 - accuracy: 0.7571\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3513 - accuracy: 0.7571\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3513 - accuracy: 0.7657\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3512 - accuracy: 0.7629\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3512 - accuracy: 0.7543\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3512 - accuracy: 0.7514\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3512 - accuracy: 0.7543\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3511 - accuracy: 0.7571\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3511 - accuracy: 0.7657\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3512 - accuracy: 0.7686\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3513 - accuracy: 0.7686\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3513 - accuracy: 0.7514\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3513 - accuracy: 0.7600\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3513 - accuracy: 0.7486\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3512 - accuracy: 0.7629\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3511 - accuracy: 0.7429\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3510 - accuracy: 0.7600\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3511 - accuracy: 0.7657\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3512 - accuracy: 0.7571\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3511 - accuracy: 0.7629\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3511 - accuracy: 0.7486\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3511 - accuracy: 0.7629\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3511 - accuracy: 0.7486\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3511 - accuracy: 0.7543\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3510 - accuracy: 0.7514\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3510 - accuracy: 0.7543\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3510 - accuracy: 0.7571\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3510 - accuracy: 0.7571\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3509 - accuracy: 0.7457\n"
     ]
    }
   ],
   "source": [
    "autoencoder_2 = Model(inputs=input_layer, outputs=decoder)\n",
    "autoencoder_2.compile(metrics=['accuracy'],loss='mean_squared_error',optimizer='adam')\n",
    "satck_2 = autoencoder_2.fit(autoencoder_2_input, autoencoder_2_input,epochs=100,batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_3_input = autoencoder_2.predict(autoencoder_2_input)\n",
    "autoencoder_3_input = np.concatenate((autoencoder_3_input, autoencoder_2_input))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "44/44 [==============================] - 1s 1ms/step - loss: 0.1772 - accuracy: 0.8429\n",
      "Epoch 2/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1771 - accuracy: 0.8486\n",
      "Epoch 3/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1768 - accuracy: 0.8429\n",
      "Epoch 4/50\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.1907 - accuracy: 0.83 - 0s 1ms/step - loss: 0.1769 - accuracy: 0.8400\n",
      "Epoch 5/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1768 - accuracy: 0.8514\n",
      "Epoch 6/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1770 - accuracy: 0.8386\n",
      "Epoch 7/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1769 - accuracy: 0.8400\n",
      "Epoch 8/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1766 - accuracy: 0.8357\n",
      "Epoch 9/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1766 - accuracy: 0.8286\n",
      "Epoch 10/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1762 - accuracy: 0.8300\n",
      "Epoch 11/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1762 - accuracy: 0.8429\n",
      "Epoch 12/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1763 - accuracy: 0.8443\n",
      "Epoch 13/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1762 - accuracy: 0.8443\n",
      "Epoch 14/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1760 - accuracy: 0.8514\n",
      "Epoch 15/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1765 - accuracy: 0.8371\n",
      "Epoch 16/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1777 - accuracy: 0.8200\n",
      "Epoch 17/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1767 - accuracy: 0.8343\n",
      "Epoch 18/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1763 - accuracy: 0.8371\n",
      "Epoch 19/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1760 - accuracy: 0.8514\n",
      "Epoch 20/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1759 - accuracy: 0.8471\n",
      "Epoch 21/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1758 - accuracy: 0.8400\n",
      "Epoch 22/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1758 - accuracy: 0.8357\n",
      "Epoch 23/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1758 - accuracy: 0.8400\n",
      "Epoch 24/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1758 - accuracy: 0.8371\n",
      "Epoch 25/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1759 - accuracy: 0.8343\n",
      "Epoch 26/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1758 - accuracy: 0.8300\n",
      "Epoch 27/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1758 - accuracy: 0.8300\n",
      "Epoch 28/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1757 - accuracy: 0.8429\n",
      "Epoch 29/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1757 - accuracy: 0.8400\n",
      "Epoch 30/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1757 - accuracy: 0.8371\n",
      "Epoch 31/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1757 - accuracy: 0.8486\n",
      "Epoch 32/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1758 - accuracy: 0.8457\n",
      "Epoch 33/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1758 - accuracy: 0.8457\n",
      "Epoch 34/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1758 - accuracy: 0.8471\n",
      "Epoch 35/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1757 - accuracy: 0.8357\n",
      "Epoch 36/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1756 - accuracy: 0.8400\n",
      "Epoch 37/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1758 - accuracy: 0.8329\n",
      "Epoch 38/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1758 - accuracy: 0.8343\n",
      "Epoch 39/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1756 - accuracy: 0.8386\n",
      "Epoch 40/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1757 - accuracy: 0.8329\n",
      "Epoch 41/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1756 - accuracy: 0.8371\n",
      "Epoch 42/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1756 - accuracy: 0.8500\n",
      "Epoch 43/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1756 - accuracy: 0.8429\n",
      "Epoch 44/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1755 - accuracy: 0.8429\n",
      "Epoch 45/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1755 - accuracy: 0.8514\n",
      "Epoch 46/50\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1755 - accuracy: 0.8500\n",
      "Epoch 47/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1755 - accuracy: 0.8443\n",
      "Epoch 48/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1755 - accuracy: 0.8543\n",
      "Epoch 49/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1755 - accuracy: 0.8443\n",
      "Epoch 50/50\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1758 - accuracy: 0.8300\n"
     ]
    }
   ],
   "source": [
    "autoencoder_3 = Model(inputs=input_layer, outputs=decoder)\n",
    "autoencoder_3.compile(metrics=['accuracy'], loss='mean_squared_error', optimizer='adam')\n",
    "satck_3 = autoencoder_3.fit(autoencoder_3_input, autoencoder_3_input, epochs=50, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(219, 8)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = Model(input_layer, encoder)\n",
    "X_ae1 = encoded.predict(X)\n",
    "X_ae1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.318800</td>\n",
       "      <td>1.074501</td>\n",
       "      <td>0.925603</td>\n",
       "      <td>0.060879</td>\n",
       "      <td>0.445664</td>\n",
       "      <td>1.522639</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.319359</td>\n",
       "      <td>1.074960</td>\n",
       "      <td>0.925644</td>\n",
       "      <td>0.060706</td>\n",
       "      <td>0.445611</td>\n",
       "      <td>1.522937</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.320340</td>\n",
       "      <td>1.075920</td>\n",
       "      <td>0.925472</td>\n",
       "      <td>0.060442</td>\n",
       "      <td>0.445836</td>\n",
       "      <td>1.523598</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.324142</td>\n",
       "      <td>1.079639</td>\n",
       "      <td>0.924848</td>\n",
       "      <td>0.059435</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>1.526168</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.328306</td>\n",
       "      <td>1.083765</td>\n",
       "      <td>0.925408</td>\n",
       "      <td>0.058882</td>\n",
       "      <td>0.450805</td>\n",
       "      <td>1.529232</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0         1         2         3         4         5         6    7\n",
       "0  0.0  1.318800  1.074501  0.925603  0.060879  0.445664  1.522639  0.0\n",
       "1  0.0  1.319359  1.074960  0.925644  0.060706  0.445611  1.522937  0.0\n",
       "2  0.0  1.320340  1.075920  0.925472  0.060442  0.445836  1.523598  0.0\n",
       "3  0.0  1.324142  1.079639  0.924848  0.059435  0.446809  1.526168  0.0\n",
       "4  0.0  1.328306  1.083765  0.925408  0.058882  0.450805  1.529232  0.0"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AEC_df = pd.DataFrame(data = X_ae1)\n",
    "AEC_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(AEC_df, Y, test_size=0.20, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 10.6264 - val_loss: 13.2338\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 10.5360 - val_loss: 13.1645\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.4580 - val_loss: 13.0987\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.3793 - val_loss: 13.0307\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.2989 - val_loss: 12.9637\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.2186 - val_loss: 12.8912\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.1326 - val_loss: 12.8123\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0392 - val_loss: 12.7254\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9429 - val_loss: 12.6371\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.8498 - val_loss: 12.5520\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.7631 - val_loss: 12.4627\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.6735 - val_loss: 12.3760\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5893 - val_loss: 12.2789\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5006 - val_loss: 12.1788\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.4137 - val_loss: 12.0560\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3140 - val_loss: 11.9416\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.2207 - val_loss: 11.8141\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.1140 - val_loss: 11.6893\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.0202 - val_loss: 11.5697\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.9406 - val_loss: 11.4437\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 8.8576 - val_loss: 11.3089\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7886 - val_loss: 11.1799\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.7254 - val_loss: 11.0509\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6724 - val_loss: 10.9309\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.6134 - val_loss: 10.8218\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.5674 - val_loss: 10.7090\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.5201 - val_loss: 10.6021\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.4864 - val_loss: 10.5066\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.4548 - val_loss: 10.4318\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4373 - val_loss: 10.3717\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.4131 - val_loss: 10.3276\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3888 - val_loss: 10.2911\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3794 - val_loss: 10.2605\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3605 - val_loss: 10.2273\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3492 - val_loss: 10.1983\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3463 - val_loss: 10.1870\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3306 - val_loss: 10.1745\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3247 - val_loss: 10.1827\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3163 - val_loss: 10.1721\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.3073 - val_loss: 10.1740\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2978 - val_loss: 10.1459\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2851 - val_loss: 10.1134\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2710 - val_loss: 10.0873\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 8.2639 - val_loss: 10.0730\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2525 - val_loss: 10.0697\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2436 - val_loss: 10.0708\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2350 - val_loss: 10.0771\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2229 - val_loss: 10.0673\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.2138 - val_loss: 10.0111\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.1999 - val_loss: 10.0046\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.1957 - val_loss: 9.9787\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.1793 - val_loss: 9.9640\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.1677 - val_loss: 9.9516\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1611 - val_loss: 9.9239\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1629 - val_loss: 9.8727\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1436 - val_loss: 9.8729\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1420 - val_loss: 9.8794\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1210 - val_loss: 9.8664\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1124 - val_loss: 9.8649\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0996 - val_loss: 9.8287\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.0950 - val_loss: 9.8087\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0791 - val_loss: 9.8156\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.0701 - val_loss: 9.8113\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0610 - val_loss: 9.7884\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.0490 - val_loss: 9.7712\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0472 - val_loss: 9.7660\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0368 - val_loss: 9.7605\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0203 - val_loss: 9.7643\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0050 - val_loss: 9.7343\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9924 - val_loss: 9.7161\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9933 - val_loss: 9.7162\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.9713 - val_loss: 9.6819\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9642 - val_loss: 9.6663\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9538 - val_loss: 9.6251\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9321 - val_loss: 9.6028\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.9331 - val_loss: 9.6365\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.9219 - val_loss: 9.6052\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 7.8919 - val_loss: 9.6186\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8885 - val_loss: 9.5924\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.8674 - val_loss: 9.5659\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8508 - val_loss: 9.5211\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8382 - val_loss: 9.4912\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8237 - val_loss: 9.4973\n",
      "Epoch 84/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8072 - val_loss: 9.4974\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7874 - val_loss: 9.5181\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7803 - val_loss: 9.5443\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7613 - val_loss: 9.5030\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7430 - val_loss: 9.4796\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7341 - val_loss: 9.4576\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7170 - val_loss: 9.4270\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.7047 - val_loss: 9.4016\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6864 - val_loss: 9.3624\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6852 - val_loss: 9.3497\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6664 - val_loss: 9.3570\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.6365 - val_loss: 9.3511\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6246 - val_loss: 9.3762\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6181 - val_loss: 9.3971\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6005 - val_loss: 9.3872\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.5805 - val_loss: 9.3604\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.5662 - val_loss: 9.3022\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.5483 - val_loss: 9.3261\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.5315 - val_loss: 9.2637\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.5275 - val_loss: 9.2356\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.4901 - val_loss: 9.2093\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.4724 - val_loss: 9.2429\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.4602 - val_loss: 9.1950\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.4403 - val_loss: 9.1214\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.4252 - val_loss: 9.1440\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.4014 - val_loss: 9.1947\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.3811 - val_loss: 9.1957\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.3760 - val_loss: 9.1803\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.3485 - val_loss: 9.1647\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.3219 - val_loss: 9.1456\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.2959 - val_loss: 9.1437\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.2883 - val_loss: 9.1915\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.2635 - val_loss: 9.1214\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.2403 - val_loss: 9.0926\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.2229 - val_loss: 9.0182\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.1913 - val_loss: 9.0090\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.1751 - val_loss: 9.0573\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.1494 - val_loss: 9.0618\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.1571 - val_loss: 9.0729\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.1151 - val_loss: 8.9945\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.0941 - val_loss: 8.9730\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.0722 - val_loss: 8.9152\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.0556 - val_loss: 8.9467\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.0559 - val_loss: 8.8926\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.0205 - val_loss: 8.9211\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9733 - val_loss: 8.9127\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9570 - val_loss: 8.9261\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.9516 - val_loss: 8.9025\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9222 - val_loss: 8.8643\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8928 - val_loss: 8.8456\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8895 - val_loss: 8.8148\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8762 - val_loss: 8.8326\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8639 - val_loss: 8.7948\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8422 - val_loss: 8.8115\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8331 - val_loss: 8.8230\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8065 - val_loss: 8.7564\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8094 - val_loss: 8.7796\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7933 - val_loss: 8.7976\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7815 - val_loss: 8.8116\n",
      "Epoch 143/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7659 - val_loss: 8.8328\n",
      "Epoch 144/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.7553 - val_loss: 8.8844\n",
      "Epoch 145/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7796 - val_loss: 8.8292\n",
      "Epoch 146/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 6.7454 - val_loss: 8.8243\n",
      "Epoch 147/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7360 - val_loss: 8.8330\n",
      "Epoch 148/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7236 - val_loss: 8.8201\n",
      "Epoch 149/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.7223 - val_loss: 8.8392\n",
      "Epoch 150/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7169 - val_loss: 8.8335\n",
      "Epoch 151/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7155 - val_loss: 8.8288\n",
      "Epoch 152/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7048 - val_loss: 8.7987\n",
      "Epoch 153/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7237 - val_loss: 8.8018\n",
      "Epoch 154/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6820 - val_loss: 8.7947\n",
      "Epoch 155/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6924 - val_loss: 8.7992\n",
      "Epoch 156/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6739 - val_loss: 8.8151\n",
      "Epoch 157/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6668 - val_loss: 8.7956\n",
      "Epoch 158/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6819 - val_loss: 8.7983\n",
      "Epoch 159/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.6547 - val_loss: 8.7836\n",
      "Epoch 160/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6626 - val_loss: 8.7825\n",
      "Epoch 161/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.6872 - val_loss: 8.7799\n",
      "Epoch 162/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6513 - val_loss: 8.7967\n",
      "Epoch 163/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6460 - val_loss: 8.7866\n",
      "Epoch 164/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6288 - val_loss: 8.7600\n",
      "Epoch 165/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.6475 - val_loss: 8.7620\n",
      "Epoch 166/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6723 - val_loss: 8.7662\n",
      "Epoch 167/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6022 - val_loss: 8.7593\n",
      "Epoch 168/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5995 - val_loss: 8.7607\n",
      "Epoch 169/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5958 - val_loss: 8.7645\n",
      "Epoch 170/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5825 - val_loss: 8.7629\n",
      "Epoch 171/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.5866 - val_loss: 8.7656\n",
      "Epoch 172/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5981 - val_loss: 8.7535\n",
      "Epoch 173/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5891 - val_loss: 8.7364\n",
      "Epoch 174/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5753 - val_loss: 8.7253\n",
      "Epoch 175/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5519 - val_loss: 8.7179\n",
      "Epoch 176/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.5596 - val_loss: 8.7204\n",
      "Epoch 177/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.5379 - val_loss: 8.7234\n",
      "Epoch 178/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.5308 - val_loss: 8.7150\n",
      "Epoch 179/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.5359 - val_loss: 8.7289\n",
      "Epoch 180/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5304 - val_loss: 8.7091\n",
      "Epoch 181/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5237 - val_loss: 8.7054\n",
      "Epoch 182/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5353 - val_loss: 8.7685\n",
      "Epoch 183/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5238 - val_loss: 8.6937\n",
      "Epoch 184/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5097 - val_loss: 8.6888\n",
      "Epoch 185/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5144 - val_loss: 8.6944\n",
      "Epoch 186/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5086 - val_loss: 8.6935\n",
      "Epoch 187/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5059 - val_loss: 8.6826\n",
      "Epoch 188/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.5153 - val_loss: 8.6865\n",
      "Epoch 189/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4899 - val_loss: 8.6751\n",
      "Epoch 190/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4772 - val_loss: 8.6658\n",
      "Epoch 191/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5272 - val_loss: 8.6501\n",
      "Epoch 192/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4806 - val_loss: 8.6263\n",
      "Epoch 193/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4664 - val_loss: 8.6320\n",
      "Epoch 194/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4637 - val_loss: 8.6200\n",
      "Epoch 195/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.4633 - val_loss: 8.6049\n",
      "Epoch 196/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4694 - val_loss: 8.6066\n",
      "Epoch 197/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4541 - val_loss: 8.5961\n",
      "Epoch 198/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4549 - val_loss: 8.6254\n",
      "Epoch 199/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4749 - val_loss: 8.5882\n",
      "Epoch 200/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4593 - val_loss: 8.5766\n",
      "Train: 6.411, Test: 8.577\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DO NOT Modify\n",
    "reg_model = Sequential()\n",
    "reg_model.add(Dense(8, input_dim=8, activation='relu'))\n",
    "reg_model.add(Dense(4, activation='relu'))\n",
    "reg_model.add(Dense(1, activation='linear'))\n",
    "reg_model.compile(loss='mae', \n",
    "                optimizer='SGD')\n",
    "\n",
    "\n",
    "history = reg_model.fit(X_train, Y_train, \n",
    "                            validation_data=(X_test, Y_test), \n",
    "                            epochs=200, verbose=1)\n",
    "\n",
    "train_mse = reg_model.evaluate(X_train, Y_train, verbose=0)\n",
    "test_mse = reg_model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))\n",
    "# plot loss during training\n",
    "y_preds=reg_model.predict(X_test)\n",
    "plt.title('Loss / Mean Squared Error')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(Y_test,y_preds)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent component analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FastICA\n",
    "ica = FastICA(n_components=8)\n",
    "X_ica = ica.fit_transform(X)\n",
    "X_ica = pd.DataFrame(data = X_ica)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.016637</td>\n",
       "      <td>-0.032390</td>\n",
       "      <td>-0.120925</td>\n",
       "      <td>-0.073528</td>\n",
       "      <td>0.032233</td>\n",
       "      <td>0.031620</td>\n",
       "      <td>0.035413</td>\n",
       "      <td>0.015851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.016648</td>\n",
       "      <td>-0.032399</td>\n",
       "      <td>-0.120950</td>\n",
       "      <td>-0.073510</td>\n",
       "      <td>0.032222</td>\n",
       "      <td>0.031528</td>\n",
       "      <td>0.035400</td>\n",
       "      <td>0.015844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.016666</td>\n",
       "      <td>-0.032414</td>\n",
       "      <td>-0.120992</td>\n",
       "      <td>-0.073480</td>\n",
       "      <td>0.032203</td>\n",
       "      <td>0.031374</td>\n",
       "      <td>0.035378</td>\n",
       "      <td>0.015832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.016739</td>\n",
       "      <td>-0.032473</td>\n",
       "      <td>-0.121160</td>\n",
       "      <td>-0.073360</td>\n",
       "      <td>0.032129</td>\n",
       "      <td>0.030761</td>\n",
       "      <td>0.035292</td>\n",
       "      <td>0.015786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.016885</td>\n",
       "      <td>-0.032591</td>\n",
       "      <td>-0.121495</td>\n",
       "      <td>-0.073121</td>\n",
       "      <td>0.031981</td>\n",
       "      <td>0.029534</td>\n",
       "      <td>0.035118</td>\n",
       "      <td>0.015693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -0.016637 -0.032390 -0.120925 -0.073528  0.032233  0.031620  0.035413   \n",
       "1 -0.016648 -0.032399 -0.120950 -0.073510  0.032222  0.031528  0.035400   \n",
       "2 -0.016666 -0.032414 -0.120992 -0.073480  0.032203  0.031374  0.035378   \n",
       "3 -0.016739 -0.032473 -0.121160 -0.073360  0.032129  0.030761  0.035292   \n",
       "4 -0.016885 -0.032591 -0.121495 -0.073121  0.031981  0.029534  0.035118   \n",
       "\n",
       "          7  \n",
       "0  0.015851  \n",
       "1  0.015844  \n",
       "2  0.015832  \n",
       "3  0.015786  \n",
       "4  0.015693  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ica.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
